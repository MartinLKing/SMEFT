%!TEX root=main.tex
\section{EFTs: Between Models and Theories} 		% (fold)
\label{sec:models} 

Philosophical approaches to scientific models have traditionally been connected to scientific theories. 
Early syntactic accounts, most prominently by logical empiricists, treated theories as axiomatic formal systems \citep[such as][]{carnap59,hempel66}
within which particular models can be articulated. 
An axiom could have only one intended model (up to isomorphism) or plenty. 
This view was criticised in part for not being an appropriate way to cast actual scientific theories. 
As a consequence, several approaches that focused on mathematical equations rather than logical propositions were developed.
They come in many variants, but generally agree that a theory is constituted by a set of related models. 
While some maintain the goal of axiomatically reconstructing theories \citep[among them][]{suppes60}, others do not, among them \citep{giere88}.
The semantic approach, as it became called, focuses on the content and representation of models rather than merely on their syntactic structure.
Here, the notion of isomorphism has played a strong role in relating the phenomena to the features of the models that describe them \citep{vf80}. 
A theory on this view could be a set of principles, core equations, or axioms from which models can be built, or it could be a set of non-hierarchically related models. These relations can be formal \citep{suppes60} or epistemic \citep{giere88}.
A purely syntactic view of theories will not allow one to describe the turn towards model-independent searches in particle physics described in Sections \ref{sec:data} and \ref{sec:eftintro}, because every QFT, including EFTs, and indeed every Lagrangian, qualifies a model of the theory and thus the distinction in practice that we are hoping to capture would simply be defined away.\footnote{It is true, the quantum field theory used on current particle physics is not axiomatized, but the syntactic understanding of model is often used in a looser sense described here, especially by those who have criticized philosophers' focus on axiomatic quantum field theory.}

In order to assess bottom-up strategies like SM-EFT, we have to look at those philosophical conceptions of models that emphasize their autonomy from the framework theory or even take them as primary and emphasize their epistemic role. 
As a bottom-up strategy, the SM-EFT does not simply extend its empirical content with new fundamental fields, but expands the standard model with higher-order terms consisting of nothing but SM fields.
The goal of such a bottom-up strategy is, initially, not to provide a however limited description of phenomena at a given scale, but to provide a rather broad theoretical framework for experimental searches that may result in constraints on BSM physics.
After discussing the philosophical literature on models and theories relevant for our problem, we will present the bottom-up SM-EFT approach in more detail in Section~\ref{sec:classification} and review a case study in Section~\ref{sec:BphysicsConcepts} before applying this discussion to extract philosophical lessons in Section~\ref{sec:analysis}.

\subsection{Models and Theories}
\label{sub:modeltheory}

In moving from syntactic to semantic views, the focus of philosophical analysis shifted from theories to models and this also informed accounts that stressed the autonomy or independence of models from theories, such as the practice-based approaches of \citet{cartwright99}, and \citet{morganmorrison}.
By and large, accounts of models have become broader and more encompassing.
A model no longer needs to be an interpretation of a formal system, a set-theoretic entity, or an isomorphic representation, but can be determined by in part by its function and use in scientific practice.
The distinction between model and theory can be one of degree more than kind, where a theory has broader empirical scope and is more highly confirmed. 
In this Section, we will analyse the distinction in the context of two approaches that have already played a role in discussions of elementary particle physics, the practice-based account of \citet{morganmorrison} and the semantic account of \citet{hartmann1998}. 
While the MaM approach emphasizes the representative and autonomous roles of models in scientific practice, Hartmann's  hierarchy of theories and models attributes a stronger role to framework theories, such as QFT, thus being somewhat closer to Suppes' version of the semantic view \citep{suppes1962}. 
As representation is a key issue, we also introduce the artefactual view of models, given by \citet{knuuttila2011,Knuuttila2017}, that explicitly does not require the accurate representation of models. 


On the MaM view, ``the rough and ready distinction followed by scientists is usually to reserve the word model for an account of a process that is less certain or incomplete in important respects. 
Then as the model is able to account for more phenomena and has survived extensive testing it evolves into a theory'' \citep[p.~18]{morganmorrison}. 
\citeauthor{morganmorrison} identify four basic elements of models: 
\begin{enumerate}
	\item They are partially autonomous from both theory and data.
	\item They are purpose-built.
	\item They are representative.
	\item They enable us to learn about the world.
\end{enumerate}
We take it to be fairly uncontroversial that models allow us to learn and that they are built with a variety of purposes in mind.
As such, we will focus our discussion on the remaining two elements.
We will first examine autonomy of models by reviewing the relations between models and theories and move to representation in Section~\ref{sub:reptarget}.
One can see that models are partially autonomous from data and theory from the way they are constructed and function.
Models are not derived entirely from theory or from data, but partly from both and their construction avails itself of scientific practices such as simplification, idealisation, and approximation.
A model is autonomous in that it can be an object of study or a tool in investigations of the world independently of its relation to the formal framework of a theory.

Let us now view the model-theory relation from the perspective of \citet{hartmann1998}, who distinguishes two kinds of theories and two kinds of models:
\begin{itemize}
\item A Type-A theory is a general background theory, quantum field theory in our case. We understand this theory to include a set of techniques like renormalization and only require that it is consistent in a physical sense. 
\item A Type-B theory is a fundamental ``model-theoretical model of a Type A theory, that is a concrete realization of the general formalism of QFT'' \citep[][p.~101]{hartmann1998}. This theory is called fundamental because ``{\it at the moment} everything that can be said about the respective domain of applicability can be captured" (ibid.) by it. 
Hartmann's own examples are QED and QCD.
\end{itemize}
All other theoretical constructs are models. 


\begin{itemize}
\item Type-A models are non-fundamental model-theoretical models. They can be simple toy models for a Type-A theory or be as specific as variants or speculative modifications of a Type-B theory. 
Hartmann's example is a quartic interaction $\phi^4$ theory, but it seems to us that also most BSM models belong in this class as long as they do not fall outside the framework of QFT. 
The important point is that these models are models from a formal point of view. 

\item A Type-B model, or phenomenological model, is ``a set of assumptions about some object or system. Some of these assumptions may be \emph{inspired} by a Type-B theory, others may even contradict the relevant theory (in case there is any)'' (ibid.). Accordingly, Type-B models are rather a residual category in Hartmann's fourfold classification. 
They are not necessarily models in a syntactic or semantic sense, but emerge from scientific practice. 
Hartmann's examples are the various models of nuclear and hadronic structure that were developed long before QCD, models which later could be integrated into this formal framework. 
\end{itemize}

Hartmann's understanding of being `fundamental' involves a relationship between the theoretical constructs and the domain of facts known at the time in such a way that if a fundamental model describes a broader domain of facts, it becomes a theory in line with Morrison's above quoted terminology. However, Hartmann seems well aware that his understanding of fundamental is quite permissive and does not support reductionism. Discussing non-renormalizable EFTs as Type B models he concludes that, if one interprets the cut-off physically as their domain of applicability, ``at a given energy scale E a suitably chosen EFT can be fundamental in the sense that (practically) nothing more can be said about the ongoing physics" (p.~104).

Hartmann's examples for Type-B models also correspond to phenomenological models in the MaM approach as examples that ``make use of a variety of theoretical and empirical assumptions'' \citep[][p.~45]{morrison99}.
Their autonomy is based on certain representative features that are not fully derivable from theory or may even go beyond the available background theory, if any.  
Hartmann's requirements for Type-B models seem lower than those of MaM because he only requires `a set of assumptions', and not any partially autonomous representative features. 
Yet his partly hierarchical account emphasizes the role of the background theory and of Type-B theories more strongly, while MaM start from the models' autonomy and their epistemic function. 
In their approach, the existence of what Hartmann calls a Type-A theory is not required---as long there is some framework or some tangible way to study the model. 
Having a well developed framework theory is not excluded in MaM, but the autonomy of models kicks in because the framework theory is insufficient to make clear predictions or to derive the model in a canonical way. This is precisely the situation in particle physics.
\footnote{Note that a hierarchical approach, in the sense of Hartmann and Suppes, also stands behind Karaca's \citeyear{karaca2013} distinction between a framework theory, a model theory (the Type-B theories, in Hartmann's sense) and phemomenological models (Type-A or Type-B models, in Hartmann's sense). \citet{karaca2018} also extends, but qualifies, a similar levelling into models of the detector etc.---which are not of concern here.}

Putting MaM alongside Hartmann's version of a semantic approach we find two, only partially overlapping uses of `model' that one can also discern in the broader philosophical literature. 
One where a model is defined as a restricted or specified part of a scientific theory that is related to a domain of phenomena, and one where a model is built in practice by studying real-world phenomena.  
Both are equally legitimate and are applied in physical practice. 
On some accounts, model building begins within an existing theoretical framework while, on others, it proceeds initially without regard for such a theoretical framework, even though models may well become theories. While MaM clearly belongs into the latter category, Hartmann's account belongs to the former, apart from his introduction of the residual class of phenomenological models. 


\subsection{Representation and Target} \label{sub:reptarget}

Let us now turn to the fourth element of MaM, representation.
In contrast to the traditional view where a model must be isomorphic to, or somehow mirror, the phenomenon, representation for MaM is instead taken as ``a kind of rendering---a partial representation that either abstracts from, or translates into another form, the real nature of the system or theory, or one that is capable of embodying only a portion of a system'' \citep[p.~27]{morganmorrison}.
Models can be representative of either the world (their ``target system'') or of theory, they can also represent both, one more than the other. 
For instance, the hydrodynamic model of the boundary layer discussed by \citep{morrison99} demonstrates how representation can happen in both directions.
Prandtl built a small water tunnel that acted as a representation of the world, which exhibited different behaviours in different regions and allowed him to construct a mathematical model.
The mathematical model in turn represented aspects of both the classical theory and the Navier-Stokes equations, neither of which could be applied directly to the real world. 
Thus, the models represented in different ways, had different targets of representation, and acted as mediators between theories and the world. 
Morrison contrasts the boundary layer model, where one has to develop a target for mathematically efficient representation, to wit, the boundary layer, with the mathematical pendulum, where the formalism of Newtonian mechanics provides all the formal tools for a de-idealisation, i.e., possible terms for the introduction of friction, finite size of the bob, etc. 
It is this emphasis on the practical aspects of representation that distinguishes MaM from Hartmann's more theory-based approach---even though Hartmann's residual category (Type-B models) is pretty elastic.

In our use of representation, we follow MaM and make no strong demands on isomorphism or accurate representation and gladly accept the important role of modelling practices, such as idealisation, approximation, and abstraction. 
Exactly how the elements of a system are mathematically represented in a given model can only be justified by some story made in a case-by-case basis. We are thus willing to endorse a rather liberal notion of representation where a representation in a scientific model is a mathematical description of a target or target system. A target (or target system) can be whatever the model is aiming at describing, which may be actual, as in the case of a model of the electron, or merely potential, as in models of BSM physics. Targets may be objects, such as fields, or descriptions of how objects interact, such as interaction vertices, and target systems may include both. Targets may be as accurate as possible, or they may be idealised, abstracted, and approximate. Again, we make no strong demands on the `mapping' relation. However, a bare minimum condition for being a model is that it must be a \textit{model of something}, and as such it must at least have some target of representation. Furthermore, its target, whatever it is, needs to be distinct and well defined, such that we can meaningfully say that it is being modelled. Having a specifiable target is then a bare necessary condition for modelhood, though it is of course not a sufficient condition.
% We take a representation to be a mathematical description of a potentially observable element of the world, which requires a sufficiently precise target to be represented.
In our specific context, these targets are elements are of physics beyond the SM, such as new fields and vertices.
Because we take a model to be a model of something, then what it represents needs to be, in a sense, distinct and well defined. 
If it is a genuine model of BSM physics, then its target is some purported phenomenon beyond the SM and what it represents needs to be distinct from the SM. 
If a BSM model has a distinct and well-defined representational target it will allow physicists to make specific predictions about new physics.
Making numerical predictions alone, however, is not sufficient for having a well-defined target. 
Lagrangians like the one in Eq.~\ref{eq:smeft} contain a large number of parameters that can be tweaked to absorb any deviation. 
% or squeeze out any desired prediction. 
The essential point of Eqs.~\ref{eq:BSM} and \ref{eq:smeft} is thus the schematic separation into a well-defined background model---the SM whose sizeable number of parameters are considered fixed by experiment---and deviations from it that are parametrized in terms of an EFT.

While the MaM approach requires that a model has some target or even representative features, others have argued  
that representation is not a characteristic feature of a scientific model.
Some practice-based approaches consider representation an accomplishment of model-users rather than a feature of the model itself.
\citet{knuuttila2011,Knuuttila2017} and \citet{teller01} have argued that there is no general account of how and in virtue of what scientific models can be said to be adequate representations. 
In a sense, anything can stand for anything and what represents something is what is chosen to represent it.
This representation can be successful or not for a given end, but whether it represents or not is not decided by features of the model or the target.
On accounts such as Knuuttila's, the key features of models are epistemic: they are tools that allow us to learn, to make good inferences, to convey useful information about the world, and the role of representation is no longer central.
Knuuttila argues that representationalist views of modelling come up against problems because they treat models as distinct objects that need to be mapped to the world in order to justify their success. 
By contrast, she develops an artefactual account that attempts to dissolve these problems by conceiving of models as epistemic tools rather than abstract objects.

This ``amounts to regarding [models] as concrete artefacts that are built by specific representational means and are constrained by their design in such a way that they facilitate the study of certain scientific questions, and learning from them by means of construction and manipulation'' \citep[p.~262]{knuuttila2011}.
She lists five characteristics of models as epistemic tools:
``(i) the constrained design of models, (ii) non-transparency of the representational means by which they are constructed, (iii) their results-orientedness, (iv) their concrete manipulability and (v) the way their justification is distributed so as to cover both the construction and the use of models'' \citep[p.~267]{knuuttila2011}.
The epistemic characteristic of models does not include accurate representation, but neither is it precluded.
Representation is always an accomplishment. 
But a model's success cannot be assessed without ``the culturally established external representational tools that enable, embody and extend scientific imagination and reasoning'' \citep[p.~1]{Knuuttila2017}. 
They ``form an ineliminable part of the model itself,'' (p. 3) not just of its description. 
``What distinguishes models from many other kinds of representations is often their systemic, autonomous nature,'' (p. 17) which they share with fictions.
Knuuttila illustrates her artefactual account with the example of highly-idealised mathematical models that are applicable in unrelated empirical contexts. 


\subsection{Effective Field Theories}\label{PhilEFT}

There already exists considerable philosophical literature on effective field theories.
Philosophical attention turned to EFTs in the late 1980s and early 1990s with the seminal papers by \citet{teller89}, \citet{cao1993}, and \citet{huggetweingard}. 
\citet{cao1993} used EFTs to articulate anti-foundationalist metaphysics where there is no final theory, but only an infinite tower of EFTs. 
From these discussions various metaphysical and ontological arguments were picked up in \citet{castellani2002} and \citet{fraser2009}, and more recently in \citep{bain2013a,Crowther2016-CROESU,Rivat2020-RIVPFO,Ruetsche2018,williams2018}. 
\citet{Rivat2020-RIVPFO} provide an excellent overview of the debates about EFTs in philosophy and note that they are primarily about issues of ontology, emergence and inter-theory relations, and fundamentality. 
A substantial part of these debates focuses on whether they are robust or fundamental enough to be worthy of philosophers' attention as compared to axiomatized approaches---which, for \citet{wallace2006} and \citet{williams2018} are still in the focus of philosophical orthodoxy. 
Williams' `effective realism' about EFT is certainly one way to justify the value of EFTs. 
\citet{fraserj2020} defends a more refined selective account of realism based on  the renormalization group. 
A detailed criticism of the various ways to identify the real parts of EFTs has been given by \citep{rivat2020a}.
Another, more instrumentalist, view was developed, initially by \citet{huggetweingard}, in response to the controversial views of \citet{cao1993}, whereby EFTs are merely tools for extending the scope of physics. 
\citet{Rivat2020-RIVPFO} describe Huggett and Weingard as saying that 
``one should not draw conclusions about the hierarchical structure of the world, the existence of a fundamental level, or the prospects of a completely unified theoretical description'' based on EFTs.
This pragmatic view is picked up on by \citet{hartmann2001}, for whom EFTs do not suggest anti-foundationalist metaphysics, nor will they eclipse models or theories, but rather that they have a role play in the search for new physics alongside models and theories.
According to \citet{Butterfield_2010}, the use of EFTs come from ``an opportunistic or instrumentalist attitude to being unconfident'' about the applicability of QFT at high energies.
\citet{Grinbaum2008-GRIOTE} also endorses this kind of view and likens the current interest in EFTs to the widespread use of S-matrix theory in the 1950s, where a clear consensus on the way forward was missing. 
With considerable foresight, Grinbaum suggests that ``perhaps something like this is happening today with EFT and the model-independent analysis of new physics'' (ibid., p.~45).
Indeed, today, physicists are unsure what the next accepted theory will be like and once again we find them turning to pragmatic benefits in the form of EFTs.

But we have to note that most of these authors have focused on EFTs in general and not specifically on the bottom-up approach of the SM-EFT.
\citet[p.~3]{Rivat2020-RIVPFO} even allude to SM-EFT but consider it as one among many black-box models. 
\citet[p.~72]{Crowther2016-CROESU} considers bottom-up EFTs as the main tool for constructing a tower of EFTs in the style of Cao and Schweber, which she does not consider ``as a threat to the possibility of there being an ultimate fundamental theory of physics'' (p.~74).  
One exception is \citet{wells2011} who examines bottom-up EFTs as a part of a general method, or mind-frame, for scientific research that could have been used, he argues, to point the way towards general relativity by predictions about the perihelion precession of Mercury already in the late 19th century. 
The in-principle feasibility of such a bottom-up approach, to his mind, suggests that physicists should focus on improving theories through EFTs and not be exclusively concerned with principles and justifications.
Wells does not discuss whether this makes EFTs or effective theories in general similar to models.

We will largely avoid issues of realism or reductionism because physicists availing themselves of SM-EFT are openly granting the reality of the SM in order to look for deviations of any kind.
Their eventual goal is to find constraints on possible models of such new physics. 
We are interested in the use of EFTs in practice and the importance of the bottom-up approach and find the modelling perspective to be more illuminating.
Thus we take our discussion to pick up on questions introduced by \citet{hartmann2001}, who takes a detailed look at the history and development of EFTs in the context of models and theories.
The metaphysical and ontological theorizing based on EFTs, which Hartmann was reluctant to do, has been strongly taken up in the last two decades, but his discussion of the roles of EFTs, models, and theories has been more quietly received.
In looking at case studies in hadron physics, he argues that EFTs produce scientific understanding of the processes they are modelling; they are simple, intuitive, and satisfy a variety of pragmatic and cognitive goals. 
There is rather an interplay between theories, models, and EFTs in physics research and distinguishing them is not always easy. 
``EFTs share many of the functions of theories and models. Like models, they provide a local, intuitive account of a given phenomenon in terms of the degrees of freedom which are relevant at the energy scale under consideration. They are relatively easy to solve and to apply, and they are heuristically useful\ldots Like theories, EFTs are part of a bigger picture or framework, from which they can be derived in a controlled way. They help to make predictions and to test the theory they relate to'' (\citeyear[p.~294]{hartmann2001}). 
Among Hartmann's examples are the above-discussed Fermi theory and the V--A theory of the weak interaction. 
% Though Hartmann primarily focuses on top-down EFTs, 
Our analysis similarly finds that the SM-EFT has complex relations to theories and models. 
While accordingly this double relationship remains, there are significant differences and we will argue that its nature may even depend on the experimental success of the SM-EFT.


